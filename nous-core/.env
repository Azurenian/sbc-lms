# PI-LMS AI Service Configuration

# External API Keys (replace with your actual keys)
GEMINI_API_KEY=AIzaSyBhMsWWmv-pnU0ADXZu4LMu6nJ0u6wBrJE
YOUTUBE_API_KEY=AIzaSyBXXmUWlBMWjqpioi2TMWvVsX7WBM0zkRc

# LLM Configuration (development - use localhost when running separately)
# OLLAMA_HOST=http://localhost:11434

# For production when both services run in docker, use:
# OLLAMA_HOST=http://ollama:11434
LLAMA_CPP_HOST=http://llama_cpp_server:8080
LLM_MODEL=llama3.2-3b.Q4_K_M.gguf
LLM_MAX_TOKENS=4000
LLM_TEMPERATURE=0.7
ENABLE_LOCAL_LLM=true

# Payload CMS Configuration (development - use localhost when vale-core runs separately)
PAYLOAD_BASE_URL=http://valecore:3000

# For production when both services run in docker, use:
# PAYLOAD_BASE_URL=http://vale-core:3000

# Environment Settings
ENVIRONMENT=development
DEBUG=true
LOG_LEVEL=info

# CORS Origins
CORS_ORIGINS=http://vale-core:3000,http://localhost:3000,http://localhost:8080,http://127.0.0.1:3000,http://127.0.0.1:8080